{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s6S1yeLmGtxv"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urls=['https://python.langchain.com/v0.2/docs/introduction/','https://python.langchain.com/v0.2/docs/tutorials/','https://python.langchain.com/v0.2/docs/how_to/','https://python.langchain.com/v0.2/docs/concepts/']\n",
        "contents=[]"
      ],
      "metadata": {
        "id": "dYJCEYClHA5w"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_content(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    content = soup.get_text()\n",
        "    contents.append(content)\n",
        "    print(f\"fetched content length is {len(soup.text)} from {url}\")\n",
        "    return content\n",
        "\n",
        "print(\"Fetching webpages...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyZXVyIbHSkP",
        "outputId": "15a6c886-06a8-4952-cbad-3ddddb382a8c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching webpages...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threads = []\n",
        "for url in urls:\n",
        "    t = threading.Thread(target=fetch_content, args=(url,))\n",
        "    threads.append(t)\n",
        "    t.start()\n",
        "\n",
        "\n",
        "for t in threads:\n",
        "    t.join()\n",
        "\n",
        "print(\"All webpages fetched!!\")\n",
        "for content in contents: # iterate through the list of contents and print each one\n",
        "  print(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3EusKDyIOBK",
        "outputId": "05c4ab30-b913-4f69-e61a-1fc2ffb2efb3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fetched content length is 9716 from https://python.langchain.com/v0.2/docs/introduction/\n",
            "fetched content length is 7921 from https://python.langchain.com/v0.2/docs/tutorials/\n",
            "fetched content length is 16958 from https://python.langchain.com/v0.2/docs/how_to/\n",
            "fetched content length is 72126 from https://python.langchain.com/v0.2/docs/concepts/\n",
            "All webpages fetched!!\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Introduction | ü¶úÔ∏èüîó LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentLangChain 0.2 is out! Leave feedback on the v0.2 docs here. You can view the v0.1 docs here.IntegrationsAPI ReferenceMorePeopleContributingTemplatesCookbooks3rd party tutorialsYouTubearXivv0.2v0.2v0.1ü¶úÔ∏èüîóLangSmithLangSmith DocsTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun LLMs locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to migrate chains to LCELHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsOverviewRelease PolicyPackagesv0.2LangChain v0.2astream_events v2ChangesSecurityIntroductionOn this pageIntroductionLangChain is a framework for developing applications powered by large language models (LLMs).LangChain simplifies every stage of the LLM application lifecycle:Development: Build your applications using LangChain's open-source building blocks, components, and third-party integrations.\n",
            "Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.Productionization: Use LangSmith to inspect, monitor and evaluate your chains, so that you can continuously optimize and deploy with confidence.Deployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Cloud.Concretely, the framework consists of the following open-source libraries:langchain-core: Base abstractions and LangChain Expression Language.langchain-community: Third party integrations.Partner packages (e.g. langchain-openai, langchain-anthropic, etc.): Some integrations have been further split into their own lightweight packages that only depend on langchain-core.langchain: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.LangGraph: Build robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph. Integrates smoothly with LangChain, but can be used without it.LangServe: Deploy LangChain chains as REST APIs.LangSmith: A developer platform that lets you debug, test, evaluate, and monitor LLM applications.noteThese docs focus on the Python LangChain library. Head here for docs on the JavaScript LangChain library.Tutorials‚ÄãIf you're looking to build something specific or are more of a hands-on learner, check out our tutorials section.\n",
            "This is the best place to get started.These are the best ones to get started with:Build a Simple LLM ApplicationBuild a ChatbotBuild an AgentIntroduction to LangGraphExplore the full list of LangChain tutorials here, and check out other LangGraph tutorials here.How-to guides‚ÄãHere you‚Äôll find short answers to ‚ÄúHow do I‚Ä¶.?‚Äù types of questions.\n",
            "These how-to guides don‚Äôt cover topics in depth ‚Äì you‚Äôll find that material in the Tutorials and the API Reference.\n",
            "However, these guides will help you quickly accomplish common tasks.Check out LangGraph-specific how-tos here.Conceptual guide‚ÄãIntroductions to all the key parts of LangChain you‚Äôll need to know! Here you'll find high level explanations of all LangChain concepts.For a deeper dive into LangGraph concepts, check out this page.API reference‚ÄãHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.Ecosystem‚Äãü¶úüõ†Ô∏è LangSmith‚ÄãTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.ü¶úüï∏Ô∏è LangGraph‚ÄãBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it.Additional resources‚ÄãSecurity‚ÄãRead up on our Security best practices to make sure you're developing safely with LangChain.Integrations‚ÄãLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of integrations.Contributing‚ÄãCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.Edit this pageWas this page helpful?You can also leave detailed feedback on GitHub.NextTutorialsTutorialsHow-to guidesConceptual guideAPI referenceEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphAdditional resourcesSecurityIntegrationsContributingCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Tutorials | ü¶úÔ∏èüîó LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentLangChain 0.2 is out! Leave feedback on the v0.2 docs here. You can view the v0.1 docs here.IntegrationsAPI ReferenceMorePeopleContributingTemplatesCookbooks3rd party tutorialsYouTubearXivv0.2v0.2v0.1ü¶úÔ∏èüîóLangSmithLangSmith DocsTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun LLMs locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to migrate chains to LCELHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsOverviewRelease PolicyPackagesv0.2LangChain v0.2astream_events v2ChangesSecurityTutorialsOn this pageTutorialsNew to LangChain or to LLM app development in general? Read this material to quickly get up and running.Basics‚ÄãBuild a Simple LLM Application with LCELBuild a ChatbotBuild vector stores and retrieversBuild an AgentWorking with external knowledge‚ÄãBuild a Retrieval Augmented Generation (RAG) ApplicationBuild a Conversational RAG ApplicationBuild a Question/Answering system over SQL dataBuild a Query Analysis SystemBuild a local RAG applicationBuild a Question Answering application over a Graph DatabaseBuild a PDF ingestion and Question/Answering systemSpecialized tasks‚ÄãBuild an Extraction ChainGenerate synthetic dataClassify text into labelsSummarize textLangGraph‚ÄãLangGraph is an extension of LangChain aimed at\n",
            "building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.LangGraph documentation is currently hosted on a separate site.\n",
            "You can peruse LangGraph tutorials here.LangSmith‚ÄãLangSmith allows you to closely trace, monitor and evaluate your LLM application.\n",
            "It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.LangSmith documentation is hosted on a separate site.\n",
            "You can peruse LangSmith tutorials here.Evaluation‚ÄãLangSmith helps you evaluate the performance of your LLM applications. The below tutorial is a great way to get started:Evaluate your LLM applicationMore‚ÄãFor more tutorials, see our cookbook section.Edit this pageWas this page helpful?You can also leave detailed feedback on GitHub.PreviousIntroductionNextBuild a Question Answering application over a Graph DatabaseBasicsWorking with external knowledgeSpecialized tasksLangGraphLangSmithEvaluationMoreCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "How-to guides | ü¶úÔ∏èüîó LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentLangChain 0.2 is out! Leave feedback on the v0.2 docs here. You can view the v0.1 docs here.IntegrationsAPI ReferenceMorePeopleContributingTemplatesCookbooks3rd party tutorialsYouTubearXivv0.2v0.2v0.1ü¶úÔ∏èüîóLangSmithLangSmith DocsTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun LLMs locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to migrate chains to LCELHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsOverviewRelease PolicyPackagesv0.2LangChain v0.2astream_events v2ChangesSecurityHow-to guidesOn this pageHow-to guidesHere you‚Äôll find answers to ‚ÄúHow do I‚Ä¶.?‚Äù types of questions.\n",
            "These guides are goal-oriented and concrete; they're meant to help you complete a specific task.\n",
            "For conceptual explanations see the Conceptual guide.\n",
            "For end-to-end walkthroughs see Tutorials.\n",
            "For comprehensive descriptions of every class and function see the API Reference.Installation‚ÄãHow to: install LangChain packagesHow to: use LangChain with different Pydantic versionsKey features‚ÄãThis highlights functionality that is core to using LangChain.How to: return structured data from a modelHow to: use a model to call toolsHow to: stream runnablesHow to: debug your LLM appsLangChain Expression Language (LCEL)‚ÄãLangChain Expression Language is a way to create arbitrary custom chains. It is built on the Runnable protocol.LCEL cheatsheet: For a quick overview of how to use the main LCEL primitives.How to: chain runnablesHow to: stream runnablesHow to: invoke runnables in parallelHow to: add default invocation args to runnablesHow to: turn any function into a runnableHow to: pass through inputs from one chain step to the nextHow to: configure runnable behavior at runtimeHow to: add message history (memory) to a chainHow to: route between sub-chainsHow to: create a dynamic (self-constructing) chainHow to: inspect runnablesHow to: add fallbacks to a runnableHow to: migrate chains to LCELHow to: pass runtime secrets to a runnableComponents‚ÄãThese are the core building blocks you can use when building applications.Prompt templates‚ÄãPrompt Templates are responsible for formatting user input into a format that can be passed to a language model.How to: use few shot examplesHow to: use few shot examples in chat modelsHow to: partially format prompt templatesHow to: compose prompts togetherExample selectors‚ÄãExample Selectors are responsible for selecting the correct few shot examples to pass to the prompt.How to: use example selectorsHow to: select examples by lengthHow to: select examples by semantic similarityHow to: select examples by semantic ngram overlapHow to: select examples by maximal marginal relevanceChat models‚ÄãChat Models are newer forms of language models that take messages in and output a message.How to: do function/tool callingHow to: get models to return structured outputHow to: cache model responsesHow to: get log probabilitiesHow to: create a custom chat model classHow to: stream a response backHow to: track token usageHow to: track response metadata across providersHow to: let your end users choose their modelHow to: use chat model to call toolsHow to: stream tool callsHow to: few shot prompt tool behaviorHow to: bind model-specific formatted toolsHow to: force a specific tool callHow to: init any model in one lineMessages‚ÄãMessages are the input and output of chat models. They have some content and a role, which describes the source of the message.How to: trim messagesHow to: filter messagesHow to: merge consecutive messages of the same typeLLMs‚ÄãWhat LangChain calls LLMs are older forms of language models that take a string in and output a string.How to: cache model responsesHow to: create a custom LLM classHow to: stream a response backHow to: track token usageHow to: work with local LLMsOutput parsers‚ÄãOutput Parsers are responsible for taking the output of an LLM and parsing into more structured format.How to: use output parsers to parse an LLM response into structured formatHow to: parse JSON outputHow to: parse XML outputHow to: parse YAML outputHow to: retry when output parsing errors occurHow to: try to fix errors in output parsingHow to: write a custom output parser classDocument loaders‚ÄãDocument Loaders are responsible for loading documents from a variety of sources.How to: load CSV dataHow to: load data from a directoryHow to: load HTML dataHow to: load JSON dataHow to: load Markdown dataHow to: load Microsoft Office dataHow to: load PDF filesHow to: write a custom document loaderText splitters‚ÄãText Splitters take a document and split into chunks that can be used for retrieval.How to: recursively split textHow to: split by HTML headersHow to: split by HTML sectionsHow to: split by characterHow to: split codeHow to: split Markdown by headersHow to: recursively split JSONHow to: split text into semantic chunksHow to: split by tokensEmbedding models‚ÄãEmbedding Models take a piece of text and create a numerical representation of it.How to: embed text dataHow to: cache embedding resultsVector stores‚ÄãVector stores are databases that can efficiently store and retrieve embeddings.How to: use a vector store to retrieve dataRetrievers‚ÄãRetrievers are responsible for taking a query and returning relevant documents.How to: use a vector store to retrieve dataHow to: generate multiple queries to retrieve data forHow to: use contextual compression to compress the data retrievedHow to: write a custom retriever classHow to: add similarity scores to retriever resultsHow to: combine the results from multiple retrieversHow to: reorder retrieved results to mitigate the \"lost in the middle\" effectHow to: generate multiple embeddings per documentHow to: retrieve the whole document for a chunkHow to: generate metadata filtersHow to: create a time-weighted retrieverHow to: use hybrid vector and keyword retrievalIndexing‚ÄãIndexing is the process of keeping your vectorstore in-sync with the underlying data source.How to: reindex data to keep your vectorstore in-sync with the underlying data sourceTools‚ÄãLangChain Tools contain a description of the tool (to pass to the language model) as well as the implementation of the function to call. Refer here for a list of pre-buit tools. How to: create toolsHow to: use built-in tools and toolkitsHow to: use chat models to call toolsHow to: pass tool outputs to chat modelsHow to: pass run time values to toolsHow to: add a human-in-the-loop for toolsHow to: handle tool errorsHow to: force models to call a toolHow to: disable parallel tool callingHow to: access the RunnableConfig from a toolHow to: stream events from a toolHow to: return artifacts from a toolHow to: convert Runnables to toolsHow to: add ad-hoc tool calling capability to modelsHow to: pass in runtime secretsMultimodal‚ÄãHow to: pass multimodal data directly to modelsHow to: use multimodal promptsAgents‚ÄãnoteFor in depth how-to guides for agents, please check out LangGraph documentation.How to: use legacy LangChain Agents (AgentExecutor)How to: migrate from legacy LangChain agents to LangGraphCallbacks‚ÄãCallbacks allow you to hook into the various stages of your LLM application's execution.How to: pass in callbacks at runtimeHow to: attach callbacks to a moduleHow to: pass callbacks into a module constructorHow to: create custom callback handlersHow to: use callbacks in async environmentsHow to: dispatch custom callback eventsCustom‚ÄãAll of LangChain components can easily be extended to support your own versions.How to: create a custom chat model classHow to: create a custom LLM classHow to: write a custom retriever classHow to: write a custom document loaderHow to: write a custom output parser classHow to: create custom callback handlersHow to: define a custom toolHow to: dispatch custom callback eventsSerialization‚ÄãHow to: save and load LangChain objectsUse cases‚ÄãThese guides cover use-case specific details.Q&A with RAG‚ÄãRetrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data.\n",
            "For a high-level tutorial on RAG, check out this guide.How to: add chat historyHow to: streamHow to: return sourcesHow to: return citationsHow to: do per-user retrievalExtraction‚ÄãExtraction is when you use LLMs to extract structured information from unstructured text.\n",
            "For a high level tutorial on extraction, check out this guide.How to: use reference examplesHow to: handle long textHow to: do extraction without using function callingChatbots‚ÄãChatbots involve using an LLM to have a conversation.\n",
            "For a high-level tutorial on building chatbots, check out this guide.How to: manage memoryHow to: do retrievalHow to: use toolsHow to: manage large chat historyQuery analysis‚ÄãQuery Analysis is the task of using an LLM to generate a query to send to a retriever.\n",
            "For a high-level tutorial on query analysis, check out this guide.How to: add examples to the promptHow to: handle cases where no queries are generatedHow to: handle multiple queriesHow to: handle multiple retrieversHow to: construct filtersHow to: deal with high cardinality categorical variablesQ&A over SQL + CSV‚ÄãYou can use LLMs to do question answering over tabular data.\n",
            "For a high-level tutorial, check out this guide.How to: use prompting to improve resultsHow to: do query validationHow to: deal with large databasesHow to: deal with CSV filesQ&A over graph databases‚ÄãYou can use an LLM to do question answering over graph databases.\n",
            "For a high-level tutorial, check out this guide.How to: map values to a databaseHow to: add a semantic layer over the databaseHow to: improve results with promptingHow to: construct knowledge graphsLangGraph‚ÄãLangGraph is an extension of LangChain aimed at\n",
            "building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.LangGraph documentation is currently hosted on a separate site.\n",
            "You can peruse LangGraph how-to guides here.LangSmith‚ÄãLangSmith allows you to closely trace, monitor and evaluate your LLM application.\n",
            "It seamlessly integrates with LangChain and LangGraph, and you can use it to inspect and debug individual steps of your chains and agents as you build.LangSmith documentation is hosted on a separate site.\n",
            "You can peruse LangSmith how-to guides here, but we'll highlight a few sections that are particularly\n",
            "relevant to LangChain below:Evaluation‚ÄãEvaluating performance is a vital part of building LLM-powered applications.\n",
            "LangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.To learn more, check out the LangSmith evaluation how-to guides.Tracing‚ÄãTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.How to: trace with LangChainHow to: add metadata and tags to tracesYou can see general tracing-related how-tos in this section of the LangSmith docs.Edit this pageWas this page helpful?You can also leave detailed feedback on GitHub.PreviousSummarize TextNextHow-to guidesInstallationKey featuresLangChain Expression Language (LCEL)ComponentsPrompt templatesExample selectorsChat modelsMessagesLLMsOutput parsersDocument loadersText splittersEmbedding modelsVector storesRetrieversIndexingToolsMultimodalAgentsCallbacksCustomSerializationUse casesQ&A with RAGExtractionChatbotsQuery analysisQ&A over SQL + CSVQ&A over graph databasesLangGraphLangSmithEvaluationTracingCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Conceptual guide | ü¶úÔ∏èüîó LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentLangChain 0.2 is out! Leave feedback on the v0.2 docs here. You can view the v0.1 docs here.IntegrationsAPI ReferenceMorePeopleContributingTemplatesCookbooks3rd party tutorialsYouTubearXivv0.2v0.2v0.1ü¶úÔ∏èüîóLangSmithLangSmith DocsTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun LLMs locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to migrate chains to LCELHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsOverviewRelease PolicyPackagesv0.2LangChain v0.2astream_events v2ChangesSecurityConceptual guideOn this pageConceptual guideThis section contains introductions to key parts of LangChain.Architecture‚ÄãLangChain as a framework consists of a number of packages.langchain-core‚ÄãThis package contains base abstractions of different components and ways to compose them together.\n",
            "The interfaces for core components like LLMs, vector stores, retrievers and more are defined here.\n",
            "No third party integrations are defined here.\n",
            "The dependencies are kept purposefully very lightweight.Partner packages‚ÄãWhile the long tail of integrations are in langchain-community, we split popular integrations into their own packages (e.g. langchain-openai, langchain-anthropic, etc).\n",
            "This was done in order to improve support for these important integrations.langchain‚ÄãThe main langchain package contains chains, agents, and retrieval strategies that make up an application's cognitive architecture.\n",
            "These are NOT third party integrations.\n",
            "All chains, agents, and retrieval strategies here are NOT specific to any one integration, but rather generic across all integrations.langchain-community‚ÄãThis package contains third party integrations that are maintained by the LangChain community.\n",
            "Key partner packages are separated out (see below).\n",
            "This contains all integrations for various components (LLMs, vector stores, retrievers).\n",
            "All dependencies in this package are optional to keep the package as lightweight as possible.langgraph‚Äãlanggraph is an extension of langchain aimed at\n",
            "building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.LangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom flows.langserve‚ÄãA package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.LangSmith‚ÄãA developer platform that lets you debug, test, evaluate, and monitor LLM applications.LangChain Expression Language (LCEL)‚ÄãLangChain Expression Language, or LCEL, is a declarative way to chain LangChain components.\n",
            "LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest ‚Äúprompt + LLM‚Äù chain to the most complex chains (we‚Äôve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:First-class streaming support\n",
            "When you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.Async support\n",
            "Any chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.Optimized parallel execution\n",
            "Whenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.Retries and fallbacks\n",
            "Configure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We‚Äôre currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.Access intermediate results\n",
            "For more complex chains it‚Äôs often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it‚Äôs available on every LangServe server.Input and output schemas\n",
            "Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.Seamless LangSmith tracing\n",
            "As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\n",
            "With LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.LCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as LLMChain and\n",
            "ConversationalRetrievalChain. Many of these legacy chains hide important details like prompts, and as a wider variety\n",
            "of viable models emerge, customization has become more and more important.If you are currently using one of these legacy chains, please see this guide for guidance on how to migrate.For guides on how to do specific tasks with LCEL, check out the relevant how-to guides.Runnable interface‚ÄãTo make it as easy as possible to create custom chains, we've implemented a \"Runnable\" protocol. Many LangChain components implement the Runnable protocol, including chat models, LLMs, output parsers, retrievers, prompt templates, and more. There are also several useful primitives for working with runnables, which you can read about below.This is a standard interface, which makes it easy to define custom chains as well as invoke them in a standard way.\n",
            "The standard interface includes:stream: stream back chunks of the responseinvoke: call the chain on an inputbatch: call the chain on a list of inputsThese also have corresponding async methods that should be used with asyncio await syntax for concurrency:astream: stream back chunks of the response asyncainvoke: call the chain on an input asyncabatch: call the chain on a list of inputs asyncastream_log: stream back intermediate steps as they happen, in addition to the final responseastream_events: beta stream events as they happen in the chain (introduced in langchain-core 0.1.14)The input type and output type varies by component:ComponentInput TypeOutput TypePromptDictionaryPromptValueChatModelSingle string, list of chat messages or a PromptValueChatMessageLLMSingle string, list of chat messages or a PromptValueStringOutputParserThe output of an LLM or ChatModelDepends on the parserRetrieverSingle stringList of DocumentsToolSingle string or dictionary, depending on the toolDepends on the toolAll runnables expose input and output schemas to inspect the inputs and outputs:input_schema: an input Pydantic model auto-generated from the structure of the Runnableoutput_schema: an output Pydantic model auto-generated from the structure of the RunnableComponents‚ÄãLangChain provides standard, extendable interfaces and external integrations for various components useful for building with LLMs.\n",
            "Some components LangChain implements, some components we rely on third-party integrations for, and others are a mix.Chat models‚ÄãLanguage models that use a sequence of messages as inputs and return chat messages as outputs (as opposed to using plain text).\n",
            "These are traditionally newer models (older models are generally LLMs, see below).\n",
            "Chat models support the assignment of distinct roles to conversation messages, helping to distinguish messages from the AI, users, and instructions such as system messages.Although the underlying models are messages in, message out, the LangChain wrappers also allow these models to take a string as input. This means you can easily use chat models in place of LLMs.When a string is passed in as input, it is converted to a HumanMessage and then passed to the underlying model.LangChain does not host any Chat Models, rather we rely on third party integrations.We have some standardized parameters when constructing ChatModels:model: the name of the modeltemperature: the sampling temperaturetimeout: request timeoutmax_tokens: max tokens to generatestop: default stop sequencesmax_retries: max number of times to retry requestsapi_key: API key for the model providerbase_url: endpoint to send requests toSome important things to note:standard params only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration for maximum output tokens, so max_tokens can't be supported on these.standard params are currently only enforced on integrations that have their own integration packages (e.g. langchain-openai, langchain-anthropic, etc.), they're not enforced on models in langchain-community.ChatModels also accept other parameters that are specific to that integration. To find all the parameters supported by a ChatModel head to the API reference for that model.infoTool Calling Some chat models have been fine-tuned for tool calling and provide a dedicated API for tool calling.\n",
            "Generally, such models are better at tool calling than non-fine-tuned models, and are recommended for use cases that require tool calling.\n",
            "Please see the tool calling section for more information.For specifics on how to use chat models, see the relevant how-to guides here.Multimodality‚ÄãSome chat models are multimodal, accepting images, audio and even video as inputs. These are still less common, meaning model providers haven't standardized on the \"best\" way to define the API. Multimodal outputs are even less common. As such, we've kept our multimodal abstractions fairly light weight and plan to further solidify the multimodal APIs and interaction patterns as the field matures.In LangChain, most chat models that support multimodal inputs also accept those values in OpenAI's content blocks format. So far this is restricted to image inputs. For models like Gemini which support video and other bytes input, the APIs also support the native, model-specific representations.For specifics on how to use multimodal models, see the relevant how-to guides here.For a full list of LangChain model providers with multimodal models, check out this table.LLMs‚ÄãcautionPure text-in/text-out LLMs tend to be older or lower-level. Many popular models are best used as chat completion models,\n",
            "even for non-chat use cases.You are probably looking for the section above instead.Language models that takes a string as input and returns a string.\n",
            "These are traditionally older models (newer models generally are Chat Models, see above).Although the underlying models are string in, string out, the LangChain wrappers also allow these models to take messages as input.\n",
            "This gives them the same interface as Chat Models.\n",
            "When messages are passed in as input, they will be formatted into a string under the hood before being passed to the underlying model.LangChain does not host any LLMs, rather we rely on third party integrations.For specifics on how to use LLMs, see the relevant how-to guides here.Messages‚ÄãSome language models take a list of messages as input and return a message.\n",
            "There are a few different types of messages.\n",
            "All messages have a role, content, and response_metadata property.The role describes WHO is saying the message.\n",
            "LangChain has different message classes for different roles.The content property describes the content of the message.\n",
            "This can be a few different things:A string (most models deal this type of content)A List of dictionaries (this is used for multimodal input, where the dictionary contains information about that input type and that input location)HumanMessage‚ÄãThis represents a message from the user.AIMessage‚ÄãThis represents a message from the model. In addition to the content property, these messages also have:response_metadataThe response_metadata property contains additional metadata about the response. The data here is often specific to each model provider.\n",
            "This is where information like log-probs and token usage may be stored.tool_callsThese represent a decision from an language model to call a tool. They are included as part of an AIMessage output.\n",
            "They can be accessed from there with the .tool_calls property.This property returns a list of ToolCalls. A ToolCall is a dictionary with the following arguments:name: The name of the tool that should be called.args: The arguments to that tool.id: The id of that tool call.SystemMessage‚ÄãThis represents a system message, which tells the model how to behave. Not every model provider supports this.ToolMessage‚ÄãThis represents the result of a tool call. In addition to role and content, this message has:a tool_call_id field which conveys the id of the call to the tool that was called to produce this result.an artifact field which can be used to pass along arbitrary artifacts of the tool execution which are useful to track but which should not be sent to the model.(Legacy) FunctionMessage‚ÄãThis is a legacy message type, corresponding to OpenAI's legacy function-calling API. ToolMessage should be used instead to correspond to the updated tool-calling API.This represents the result of a function call. In addition to role and content, this message has a name parameter which conveys the name of the function that was called to produce this result.Prompt templates‚ÄãPrompt templates help to translate user input and parameters into instructions for a language model.\n",
            "This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.Prompt Templates take as input a dictionary, where each key represents a variable in the prompt template to fill in.Prompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or a list of messages.\n",
            "The reason this PromptValue exists is to make it easy to switch between strings and messages.There are a few different types of prompt templates:String PromptTemplates‚ÄãThese prompt templates are used to format a single string, and generally are used for simpler inputs.\n",
            "For example, a common way to construct and use a PromptTemplate is as follows:from langchain_core.prompts import PromptTemplateprompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")prompt_template.invoke({\"topic\": \"cats\"})API Reference:PromptTemplateChatPromptTemplates‚ÄãThese prompt templates are used to format a list of messages. These \"templates\" consist of a list of templates themselves.\n",
            "For example, a common way to construct and use a ChatPromptTemplate is as follows:from langchain_core.prompts import ChatPromptTemplateprompt_template = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant\"),    (\"user\", \"Tell me a joke about {topic}\")])prompt_template.invoke({\"topic\": \"cats\"})API Reference:ChatPromptTemplateIn the above example, this ChatPromptTemplate will construct two messages when called.\n",
            "The first is a system message, that has no variables to format.\n",
            "The second is a HumanMessage, and will be formatted by the topic variable the user passes in.MessagesPlaceholder‚ÄãThis prompt template is responsible for adding a list of messages in a particular place.\n",
            "In the above ChatPromptTemplate, we saw how we could format two messages, each one a string.\n",
            "But what if we wanted the user to pass in a list of messages that we would slot into a particular spot?\n",
            "This is how you use MessagesPlaceholder.from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_core.messages import HumanMessageprompt_template = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant\"),    MessagesPlaceholder(\"msgs\")])prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})API Reference:ChatPromptTemplate | MessagesPlaceholder | HumanMessageThis will produce a list of two messages, the first one being a system message, and the second one being the HumanMessage we passed in.\n",
            "If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in).\n",
            "This is useful for letting a list of messages be slotted into a particular spot.An alternative way to accomplish the same thing without using the MessagesPlaceholder class explicitly is:prompt_template = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant\"),    (\"placeholder\", \"{msgs}\") # <-- This is the changed part])For specifics on how to use prompt templates, see the relevant how-to guides here.Example selectors‚ÄãOne common prompting technique for achieving better performance is to include examples as part of the prompt.\n",
            "This gives the language model concrete examples of how it should behave.\n",
            "Sometimes these examples are hardcoded into the prompt, but for more advanced situations it may be nice to dynamically select them.\n",
            "Example Selectors are classes responsible for selecting and then formatting examples into prompts.For specifics on how to use example selectors, see the relevant how-to guides here.Output parsers‚ÄãnoteThe information here refers to parsers that take a text output from a model try to parse it into a more structured representation.\n",
            "More and more models are supporting function (or tool) calling, which handles this automatically.\n",
            "It is recommended to use function/tool calling rather than output parsing.\n",
            "See documentation for that here.Responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks.\n",
            "Useful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs.LangChain has lots of different types of output parsers. This is a list of output parsers LangChain supports. The table below has various pieces of information:Name: The name of the output parserSupports Streaming: Whether the output parser supports streaming.Has Format Instructions: Whether the output parser has format instructions. This is generally available except when (a) the desired schema is not specified in the prompt but rather in other parameters (like OpenAI function calling), or (b) when the OutputParser wraps another OutputParser.Calls LLM: Whether this output parser itself calls an LLM. This is usually only done by output parsers that attempt to correct misformatted output.Input Type: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need a message with specific kwargs.Output Type: The output type of the object returned by the parser.Description: Our commentary on this output parser and when to use it.NameSupports StreamingHas Format InstructionsCalls LLMInput TypeOutput TypeDescriptionJSON‚úÖ‚úÖstr | MessageJSON objectReturns a JSON object as specified. You can specify a Pydantic model and it will return JSON for that model. Probably the most reliable output parser for getting structured data that does NOT use function calling.XML‚úÖ‚úÖstr | MessagedictReturns a dictionary of tags. Use when XML output is needed. Use with models that are good at writing XML (like Anthropic's).CSV‚úÖ‚úÖstr | MessageList[str]Returns a list of comma separated values.OutputFixing‚úÖstr | MessageWraps another output parser. If that output parser errors, then this will pass the error message and the bad output to an LLM and ask it to fix the output.RetryWithError‚úÖstr | MessageWraps another output parser. If that output parser errors, then this will pass the original inputs, the bad output, and the error message to an LLM and ask it to fix it. Compared to OutputFixingParser, this one also sends the original instructions.Pydantic‚úÖstr | Messagepydantic.BaseModelTakes a user defined Pydantic model and returns data in that format.YAML‚úÖstr | Messagepydantic.BaseModelTakes a user defined Pydantic model and returns data in that format. Uses YAML to encode it.PandasDataFrame‚úÖstr | MessagedictUseful for doing operations with pandas DataFrames.Enum‚úÖstr | MessageEnumParses response into one of the provided enum values.Datetime‚úÖstr | Messagedatetime.datetimeParses response into a datetime string.Structured‚úÖstr | MessageDict[str, str]An output parser that returns structured information. It is less powerful than other output parsers since it only allows for fields to be strings. This can be useful when you are working with smaller LLMs.For specifics on how to use output parsers, see the relevant how-to guides here.Chat history‚ÄãMost LLM applications have a conversational interface.\n",
            "An essential component of a conversation is being able to refer to information introduced earlier in the conversation.\n",
            "At bare minimum, a conversational system should be able to access some window of past messages directly.The concept of ChatHistory refers to a class in LangChain which can be used to wrap an arbitrary chain.\n",
            "This ChatHistory will keep track of inputs and outputs of the underlying chain, and append them as messages to a message database.\n",
            "Future interactions will then load those messages and pass them into the chain as part of the input.Documents‚ÄãA Document object in LangChain contains information about some data. It has two attributes:page_content: str: The content of this document. Currently is only a string.metadata: dict: Arbitrary metadata associated with this document. Can track the document id, file name, etc.Document loaders‚ÄãThese classes load Document objects. LangChain has hundreds of integrations with various data sources to load data from: Slack, Notion, Google Drive, etc.Each DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the .load method.\n",
            "An example use case is as follows:from langchain_community.document_loaders.csv_loader import CSVLoaderloader = CSVLoader(    ...  # <-- Integration specific parameters here)data = loader.load()API Reference:CSVLoaderFor specifics on how to use document loaders, see the relevant how-to guides here.Text splitters‚ÄãOnce you've loaded documents, you'll often want to transform them to better suit your application. The simplest example is you may want to split a long document into smaller chunks that can fit into your model's context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.When you want to deal with long pieces of text, it is necessary to split up that text into chunks. As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"semantically related\" means could depend on the type of text. This notebook showcases several ways to do that.At a high level, text splitters work as following:Split the text up into small, semantically meaningful chunks (often sentences).Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).That means there are two different axes along which you can customize your text splitter:How the text is splitHow the chunk size is measuredFor specifics on how to use text splitters, see the relevant how-to guides here.Embedding models‚ÄãEmbedding models create a vector representation of a piece of text. You can think of a vector as an array of numbers that captures the semantic meaning of the text.\n",
            "By representing the text in this way, you can perform mathematical operations that allow you to do things like search for other pieces of text that are most similar in meaning.\n",
            "These natural language search capabilities underpin many types of context retrieval,\n",
            "where we provide an LLM with the relevant data it needs to effectively respond to a query.The Embeddings class is a class designed for interfacing with text embedding models. There are many different embedding model providers (OpenAI, Cohere, Hugging Face, etc) and local models, and this class is designed to provide a standard interface for all of them.The base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query. The former takes as input multiple texts, while the latter takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself).For specifics on how to use embedding models, see the relevant how-to guides here.Vector stores‚ÄãOne of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors,\n",
            "and then at query time to embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query.\n",
            "A vector store takes care of storing embedded data and performing vector search for you.Most vector stores can also store metadata about embedded vectors and support filtering on that metadata before\n",
            "similarity search, allowing you more control over returned documents.Vector stores can be converted to the retriever interface by doing:vectorstore = MyVectorStore()retriever = vectorstore.as_retriever()For specifics on how to use vector stores, see the relevant how-to guides here.Retrievers‚ÄãA retriever is an interface that returns documents given an unstructured query.\n",
            "It is more general than a vector store.\n",
            "A retriever does not need to be able to store documents, only to return (or retrieve) them.\n",
            "Retrievers can be created from vector stores, but are also broad enough to include Wikipedia search and Amazon Kendra.Retrievers accept a string query as input and return a list of Document's as output.For specifics on how to use retrievers, see the relevant how-to guides here.Tools‚ÄãTools are utilities designed to be called by a model: their inputs are designed to be generated by models, and their outputs are designed to be passed back to models.\n",
            "Tools are needed whenever you want a model to control parts of your code or call out to external APIs.A tool consists of:The name of the tool.A description of what the tool does.A JSON schema defining the inputs to the tool.A function (and, optionally, an async variant of the function).When a tool is bound to a model, the name, description and JSON schema are provided as context to the model.\n",
            "Given a list of tools and a set of instructions, a model can request to call one or more tools with specific inputs.\n",
            "Typical usage may look like the following:tools = [...] # Define a list of toolsllm_with_tools = llm.bind_tools(tools)ai_msg = llm_with_tools.invoke(\"do xyz...\")  # AIMessage(tool_calls=[ToolCall(...), ...], ...)The AIMessage returned from the model MAY have tool_calls associated with it.\n",
            "Read this guide for more information on what the response type may look like.Once the chosen tools are invoked, the results can be passed back to the model so that it can complete whatever task\n",
            "it's performing.\n",
            "There are generally two different ways to invoke the tool and pass back the response:Invoke with just the arguments‚ÄãWhen you invoke a tool with just the arguments, you will get back the raw tool output (usually a string).\n",
            "This generally looks like:# You will want to previously check that the LLM returned tool callstool_call = ai_msg.tool_calls[0]  # ToolCall(args={...}, id=..., ...)tool_output = tool.invoke(tool_call[\"args\"])tool_message = ToolMessage(content=tool_output, tool_call_id=tool_call[\"id\"], name=tool_call[\"name\"])Note that the content field will generally be passed back to the model.\n",
            "If you do not want the raw tool response to be passed to the model, but you still want to keep it around,\n",
            "you can transform the tool output but also pass it as an artifact (read more about ToolMessage.artifact here)... # Same code as aboveresponse_for_llm = transform(response)tool_message = ToolMessage(content=response_for_llm, tool_call_id=tool_call[\"id\"], name=tool_call[\"name\"], artifact=tool_output)Invoke with ToolCall‚ÄãThe other way to invoke a tool is to call it with the full ToolCall that was generated by the model.\n",
            "When you do this, the tool will return a ToolMessage.\n",
            "The benefits of this are that you don't have to write the logic yourself to transform the tool output into a ToolMessage.\n",
            "This generally looks like:tool_call = ai_msg.tool_calls[0]  # ToolCall(args={...}, id=..., ...)tool_message = tool.invoke(tool_call)# -> ToolMessage(content=\"tool result foobar...\", tool_call_id=..., name=\"tool_name\")If you are invoking the tool this way and want to include an artifact for the ToolMessage, you will need to have the tool return two things.\n",
            "Read more about defining tools that return artifacts here.Best practices‚ÄãWhen designing tools to be used by a model, it is important to keep in mind that:Chat models that have explicit tool-calling APIs will be better at tool calling than non-fine-tuned models.Models will perform better if the tools have well-chosen names, descriptions, and JSON schemas. This another form of prompt engineering.Simple, narrowly scoped tools are easier for models to use than complex tools.Related‚ÄãFor specifics on how to use tools, see the tools how-to guides.To use a pre-built tool, see the tool integration docs.Toolkits‚ÄãToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.All Toolkits expose a get_tools method which returns a list of tools.\n",
            "You can therefore do:# Initialize a toolkittoolkit = ExampleTookit(...)# Get list of toolstools = toolkit.get_tools()Agents‚ÄãBy themselves, language models can't take actions - they just output text.\n",
            "A big use case for LangChain is creating agents.\n",
            "Agents are systems that use an LLM as a reasoning engine to determine which actions to take and what the inputs to those actions should be.\n",
            "The results of those actions can then be fed back into the agent and it determine whether more actions are needed, or whether it is okay to finish.LangGraph is an extension of LangChain specifically aimed at creating highly controllable and customizable agents.\n",
            "Please check out that documentation for a more in depth overview of agent concepts.There is a legacy agent concept in LangChain that we are moving towards deprecating: AgentExecutor.\n",
            "AgentExecutor was essentially a runtime for agents.\n",
            "It was a great place to get started, however, it was not flexible enough as you started to have more customized agents.\n",
            "In order to solve that we built LangGraph to be this flexible, highly-controllable runtime.If you are still using AgentExecutor, do not fear: we still have a guide on how to use AgentExecutor.\n",
            "It is recommended, however, that you start to transition to LangGraph.\n",
            "In order to assist in this we have put together a transition guide on how to do so.ReAct agents‚ÄãOne popular architecture for building agents is ReAct.\n",
            "ReAct combines reasoning and acting in an iterative process - in fact the name \"ReAct\" stands for \"Reason\" and \"Act\".The general flow looks like this:The model will \"think\" about what step to take in response to an input and any previous observations.The model will then choose an action from available tools (or choose to respond to the user).The model will generate arguments to that tool.The agent runtime (executor) will parse out the chosen tool and call it with the generated arguments.The executor will return the results of the tool call back to the model as an observation.This process repeats until the agent chooses to respond.There are general prompting based implementations that do not require any model-specific features, but the most\n",
            "reliable implementations use features like tool calling to reliably format outputs\n",
            "and reduce variance.Please see the LangGraph documentation for more information,\n",
            "or this how-to guide for specific information on migrating to LangGraph.Callbacks‚ÄãLangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.You can subscribe to these events by using the callbacks argument available throughout the API. This argument is list of handler objects, which are expected to implement one or more of the methods described below in more detail.Callback Events‚ÄãEventEvent TriggerAssociated MethodChat model startWhen a chat model startson_chat_model_startLLM startWhen a llm startson_llm_startLLM new tokenWhen an llm OR chat model emits a new tokenon_llm_new_tokenLLM endsWhen an llm OR chat model endson_llm_endLLM errorsWhen an llm OR chat model errorson_llm_errorChain startWhen a chain starts runningon_chain_startChain endWhen a chain endson_chain_endChain errorWhen a chain errorson_chain_errorTool startWhen a tool starts runningon_tool_startTool endWhen a tool endson_tool_endTool errorWhen a tool errorson_tool_errorAgent actionWhen an agent takes an actionon_agent_actionAgent finishWhen an agent endson_agent_finishRetriever startWhen a retriever startson_retriever_startRetriever endWhen a retriever endson_retriever_endRetriever errorWhen a retriever errorson_retriever_errorTextWhen arbitrary text is runon_textRetryWhen a retry event is runon_retryCallback handlers‚ÄãCallback handlers can either be sync or async:Sync callback handlers implement the BaseCallbackHandler interface.Async callback handlers implement the AsyncCallbackHandler interface.During run-time LangChain configures an appropriate callback manager (e.g., CallbackManager or AsyncCallbackManager which will be responsible for calling the appropriate method on each \"registered\" callback handler when the event is triggered.Passing callbacks‚ÄãThe callbacks property is available on most objects throughout the API (Models, Tools, Agents, etc.) in two different places:The callbacks are available on most objects throughout the API (Models, Tools, Agents, etc.) in two different places:Request time callbacks: Passed at the time of the request in addition to the input data.\n",
            "Available on all standard Runnable objects. These callbacks are INHERITED by all children\n",
            "of the object they are defined on. For example, chain.invoke({\"number\": 25}, {\"callbacks\": [handler]}).Constructor callbacks: chain = TheNameOfSomeChain(callbacks=[handler]). These callbacks\n",
            "are passed as arguments to the constructor of the object. The callbacks are scoped\n",
            "only to the object they are defined on, and are not inherited by any children of the object.dangerConstructor callbacks are scoped only to the object they are defined on. They are not inherited by children\n",
            "of the object.If you're creating a custom chain or runnable, you need to remember to propagate request time\n",
            "callbacks to any child objects.Async in Python<=3.10Any RunnableLambda, a RunnableGenerator, or Tool that invokes other runnables\n",
            "and is running async in python<=3.10, will have to propagate callbacks to child\n",
            "objects manually. This is because LangChain cannot automatically propagate\n",
            "callbacks to child objects in this case.This is a common reason why you may fail to see events being emitted from custom\n",
            "runnables or tools.For specifics on how to use callbacks, see the relevant how-to guides here.Techniques‚ÄãStreaming‚ÄãIndividual LLM calls often run for much longer than traditional resource requests.\n",
            "This compounds when you build more complex chains or agents that require multiple reasoning steps.Fortunately, LLMs generate output iteratively, which means it's possible to show sensible intermediate results\n",
            "before the final response is ready. Consuming output as soon as it becomes available has therefore become a vital part of the UX\n",
            "around building apps with LLMs to help alleviate latency issues, and LangChain aims to have first-class support for streaming.Below, we'll discuss some concepts and considerations around streaming in LangChain..stream() and .astream()‚ÄãMost modules in LangChain include the .stream() method (and the equivalent .astream() method for async environments) as an ergonomic streaming interface.\n",
            ".stream() returns an iterator, which you can consume with a simple for loop. Here's an example with a chat model:from langchain_anthropic import ChatAnthropicmodel = ChatAnthropic(model=\"claude-3-sonnet-20240229\")for chunk in model.stream(\"what color is the sky?\"):    print(chunk.content, end=\"|\", flush=True)API Reference:ChatAnthropicFor models (or other components) that don't support streaming natively, this iterator would just yield a single chunk, but\n",
            "you could still use the same general pattern when calling them. Using .stream() will also automatically call the model in streaming mode\n",
            "without the need to provide additional config.The type of each outputted chunk depends on the type of component - for example, chat models yield AIMessageChunks.\n",
            "Because this method is part of LangChain Expression Language,\n",
            "you can handle formatting differences from different outputs using an output parser to transform\n",
            "each yielded chunk.You can check out this guide for more detail on how to use .stream()..astream_events()‚ÄãWhile the .stream() method is intuitive, it can only return the final generated value of your chain. This is fine for single LLM calls,\n",
            "but as you build more complex chains of several LLM calls together, you may want to use the intermediate values of\n",
            "the chain alongside the final output - for example, returning sources alongside the final generation when building a chat\n",
            "over documents app.There are ways to do this using callbacks, or by constructing your chain in such a way that it passes intermediate\n",
            "values to the end with something like chained .assign() calls, but LangChain also includes an\n",
            ".astream_events() method that combines the flexibility of callbacks with the ergonomics of .stream(). When called, it returns an iterator\n",
            "which yields various types of events that you can filter and process according\n",
            "to the needs of your project.Here's one small example that prints just events containing streamed chat model output:from langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_anthropic import ChatAnthropicmodel = ChatAnthropic(model=\"claude-3-sonnet-20240229\")prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")parser = StrOutputParser()chain = prompt | model | parserasync for event in chain.astream_events({\"topic\": \"parrot\"}, version=\"v2\"):    kind = event[\"event\"]    if kind == \"on_chat_model_stream\":        print(event, end=\"|\", flush=True)API Reference:StrOutputParser | ChatPromptTemplate | ChatAnthropicYou can roughly think of it as an iterator over callback events (though the format differs) - and you can use it on almost all LangChain components!See this guide for more detailed information on how to use .astream_events(),\n",
            "including a table listing available events.Callbacks‚ÄãThe lowest level way to stream outputs from LLMs in LangChain is via the callbacks system. You can pass a\n",
            "callback handler that handles the on_llm_new_token event into LangChain components. When that component is invoked, any\n",
            "LLM or chat model contained in the component calls\n",
            "the callback with the generated token. Within the callback, you could pipe the tokens into some other destination, e.g. a HTTP response.\n",
            "You can also handle the on_llm_end event to perform any necessary cleanup.You can see this how-to section for more specifics on using callbacks.Callbacks were the first technique for streaming introduced in LangChain. While powerful and generalizable,\n",
            "they can be unwieldy for developers. For example:You need to explicitly initialize and manage some aggregator or other stream to collect results.The execution order isn't explicitly guaranteed, and you could theoretically have a callback run after the .invoke() method finishes.Providers would often make you pass an additional parameter to stream outputs instead of returning them all at once.You would often ignore the result of the actual model call in favor of callback results.Tokens‚ÄãThe unit that most model providers use to measure input and output is via a unit called a token.\n",
            "Tokens are the basic units that language models read and generate when processing or producing text.\n",
            "The exact definition of a token can vary depending on the specific way the model was trained -\n",
            "for instance, in English, a token could be a single word like \"apple\", or a part of a word like \"app\".When you send a model a prompt, the words and characters in the prompt are encoded into tokens using a tokenizer.\n",
            "The model then streams back generated output tokens, which the tokenizer decodes into human-readable text.\n",
            "The below example shows how OpenAI models tokenize LangChain is cool!:You can see that it gets split into 5 different tokens, and that the boundaries between tokens are not exactly the same as word boundaries.The reason language models use tokens rather than something more immediately intuitive like \"characters\"\n",
            "has to do with how they process and understand text. At a high-level, language models iteratively predict their next generated output based on\n",
            "the initial input and their previous generations. Training the model using tokens language models to handle linguistic\n",
            "units (like words or subwords) that carry meaning, rather than individual characters, which makes it easier for the model\n",
            "to learn and understand the structure of the language, including grammar and context.\n",
            "Furthermore, using tokens can also improve efficiency, since the model processes fewer units of text compared to character-level processing.Structured output‚ÄãLLMs are capable of generating arbitrary text. This enables the model to respond appropriately to a wide\n",
            "range of inputs, but for some use-cases, it can be useful to constrain the LLM's output\n",
            "to a specific format or structure. This is referred to as structured output.For example, if the output is to be stored in a relational database,\n",
            "it is much easier if the model generates output that adheres to a defined schema or format.\n",
            "Extracting specific information from unstructured text is another\n",
            "case where this is particularly useful. Most commonly, the output format will be JSON,\n",
            "though other formats such as YAML can be useful too. Below, we'll discuss\n",
            "a few ways to get structured output from models in LangChain..with_structured_output()‚ÄãFor convenience, some LangChain chat models support a .with_structured_output()\n",
            "method. This method only requires a schema as input, and returns a dict or Pydantic object.\n",
            "Generally, this method is only present on models that support one of the more advanced methods described below,\n",
            "and will use one of them under the hood. It takes care of importing a suitable output parser and\n",
            "formatting the schema in the right format for the model.Here's an example:from typing import Optionalfrom langchain_core.pydantic_v1 import BaseModel, Fieldclass Joke(BaseModel):    \"\"\"Joke to tell user.\"\"\"    setup: str = Field(description=\"The setup of the joke\")    punchline: str = Field(description=\"The punchline to the joke\")    rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")structured_llm = llm.with_structured_output(Joke)structured_llm.invoke(\"Tell me a joke about cats\")Joke(setup='Why was the cat sitting on the computer?', punchline='To keep an eye on the mouse!', rating=None)We recommend this method as a starting point when working with structured output:It uses other model-specific features under the hood, without the need to import an output parser.For the models that use tool calling, no special prompting is needed.If multiple underlying techniques are supported, you can supply a method parameter to\n",
            "toggle which one is used.You may want or need to use other techniques if:The chat model you are using does not support tool calling.You are working with very complex schemas and the model is having trouble generating outputs that conform.For more information, check out this how-to guide.You can also check out this table for a list of models that support\n",
            "with_structured_output().Raw prompting‚ÄãThe most intuitive way to get a model to structure output is to ask nicely.\n",
            "In addition to your query, you can give instructions describing what kind of output you'd like, then\n",
            "parse the output using an output parser to convert the raw\n",
            "model message or string output into something more easily manipulated.The biggest benefit to raw prompting is its flexibility:Raw prompting does not require any special model features, only sufficient reasoning capability to understand\n",
            "the passed schema.You can prompt for any format you'd like, not just JSON. This can be useful if the model you\n",
            "are using is more heavily trained on a certain type of data, such as XML or YAML.However, there are some drawbacks too:LLMs are non-deterministic, and prompting a LLM to consistently output data in the exactly correct format\n",
            "for smooth parsing can be surprisingly difficult and model-specific.Individual models have quirks depending on the data they were trained on, and optimizing prompts can be quite difficult.\n",
            "Some may be better at interpreting JSON schema, others may be best with TypeScript definitions,\n",
            "and still others may prefer XML.While features offered by model providers may increase reliability, prompting techniques remain important for tuning your\n",
            "results no matter which method you choose.JSON mode‚ÄãSome models, such as Mistral, OpenAI,\n",
            "Together AI and Ollama,\n",
            "support a feature called JSON mode, usually enabled via config.When enabled, JSON mode will constrain the model's output to always be some sort of valid JSON.\n",
            "Often they require some custom prompting, but it's usually much less burdensome than completely raw prompting and\n",
            "more along the lines of, \"you must always return JSON\". The output also generally easier to parse.It's also generally simpler to use directly and more commonly available than tool calling, and can give\n",
            "more flexibility around prompting and shaping results than tool calling.Here's an example:from langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIfrom langchain.output_parsers.json import SimpleJsonOutputParsermodel = ChatOpenAI(    model=\"gpt-4o\",    model_kwargs={ \"response_format\": { \"type\": \"json_object\" } },)prompt = ChatPromptTemplate.from_template(    \"Answer the user's question to the best of your ability.\"    'You must always output a JSON object with an \"answer\" key and a \"followup_question\" key.'    \"{question}\")chain = prompt | model | SimpleJsonOutputParser()chain.invoke({ \"question\": \"What is the powerhouse of the cell?\" })API Reference:ChatPromptTemplate | ChatOpenAI | SimpleJsonOutputParser{'answer': 'The powerhouse of the cell is the mitochondrion. It is responsible for producing energy in the form of ATP through cellular respiration.', 'followup_question': 'Would you like to know more about how mitochondria produce energy?'}For a full list of model providers that support JSON mode, see this table.Function/tool calling‚ÄãinfoWe use the term tool calling interchangeably with function calling. Although\n",
            "function calling is sometimes meant to refer to invocations of a single function,\n",
            "we treat all models as though they can return multiple tool or function calls in\n",
            "each messageTool calling allows a model to respond to a given prompt by generating output that\n",
            "matches a user-defined schema. While the name implies that the model is performing\n",
            "some action, this is actually not the case! The model is coming up with the\n",
            "arguments to a tool, and actually running the tool (or not) is up to the user -\n",
            "for example, if you want to extract output matching some schema\n",
            "from unstructured text, you could give the model an \"extraction\" tool that takes\n",
            "parameters matching the desired schema, then treat the generated output as your final\n",
            "result.For models that support it, tool calling can be very convenient. It removes the\n",
            "guesswork around how best to prompt schemas in favor of a built-in model feature. It can also\n",
            "more naturally support agentic flows, since you can just pass multiple tool schemas instead\n",
            "of fiddling with enums or unions.Many LLM providers, including Anthropic,\n",
            "Cohere, Google,\n",
            "Mistral, OpenAI, and others,\n",
            "support variants of a tool calling feature. These features typically allow requests\n",
            "to the LLM to include available tools and their schemas, and for responses to include\n",
            "calls to these tools. For instance, given a search engine tool, an LLM might handle a\n",
            "query by first issuing a call to the search engine. The system calling the LLM can\n",
            "receive the tool call, execute it, and return the output to the LLM to inform its\n",
            "response. LangChain includes a suite of built-in tools\n",
            "and supports several methods for defining your own custom tools.LangChain provides a standardized interface for tool calling that is consistent across different models.The standard interface consists of:ChatModel.bind_tools(): a method for specifying which tools are available for a model to call. This method accepts LangChain tools here.AIMessage.tool_calls: an attribute on the AIMessage returned from the model for accessing the tool calls requested by the model.The following how-to guides are good practical resources for using function/tool calling:How to return structured data from an LLMHow to use a model to call toolsFor a full list of model providers that support tool calling, see this table.Retrieval‚ÄãLLMs are trained on a large but fixed dataset, limiting their ability to reason over private or recent information. Fine-tuning an LLM with specific facts is one way to mitigate this, but is often poorly suited for factual recall and can be costly.\n",
            "Retrieval is the process of providing relevant information to an LLM to improve its response for a given input. Retrieval augmented generation (RAG) is the process of grounding the LLM generation (output) using the retrieved information.tipSee our RAG from Scratch code and video series.For a high-level guide on retrieval, see this tutorial on RAG.RAG is only as good as the retrieved documents‚Äô relevance and quality. Fortunately, an emerging set of techniques can be employed to design and improve RAG systems. We've focused on taxonomizing and summarizing many of these techniques (see below figure) and will share some high-level strategic guidance in the following sections.\n",
            "You can and should experiment with using different pieces together. You might also find this LangSmith guide useful for showing how to evaluate different iterations of your app.Query Translation‚ÄãFirst, consider the user input(s) to your RAG system. Ideally, a RAG system can handle a wide range of inputs, from poorly worded questions to complex multi-part queries.\n",
            "Using an LLM to review and optionally modify the input is the central idea behind query translation. This serves as a general buffer, optimizing raw user inputs for your retrieval system.\n",
            "For example, this can be as simple as extracting keywords or as complex as generating multiple sub-questions for a complex query.NameWhen to useDescriptionMulti-queryWhen you need to cover multiple perspectives of a question.Rewrite the user question from multiple perspectives, retrieve documents for each rewritten question, return the unique documents for all queries.DecompositionWhen a question can be broken down into smaller subproblems.Decompose a question into a set of subproblems / questions, which can either be solved sequentially (use the answer from first + retrieval to answer the second) or in parallel (consolidate each answer into final answer).Step-backWhen a higher-level conceptual understanding is required.First prompt the LLM to ask a generic step-back question about higher-level concepts or principles, and retrieve relevant facts about them. Use this grounding to help answer the user question.HyDEIf you have challenges retrieving relevant documents using the raw user inputs.Use an LLM to convert questions into hypothetical documents that answer the question. Use the embedded hypothetical documents to retrieve real documents with the premise that doc-doc similarity search can produce more relevant matches.tipSee our RAG from Scratch videos for a few different specific approaches:Multi-queryDecompositionStep-backHyDERouting‚ÄãSecond, consider the data sources available to your RAG system. You want to query across more than one database or across structured and unstructured data sources. Using an LLM to review the input and route it to the appropriate data source is a simple and effective approach for querying across sources.NameWhen to useDescriptionLogical routingWhen you can prompt an LLM with rules to decide where to route the input.Logical routing can use an LLM to reason about the query and choose which datastore is most appropriate.Semantic routingWhen semantic similarity is an effective way to determine where to route the input.Semantic routing embeds both query and, typically a set of prompts. It then chooses the appropriate prompt based upon similarity.tipSee our RAG from Scratch video on routing.  Query Construction‚ÄãThird, consider whether any of your data sources require specific query formats. Many structured databases use SQL. Vector stores often have specific syntax for applying keyword filters to document metadata. Using an LLM to convert a natural language query into a query syntax is a popular and powerful approach.\n",
            "In particular, text-to-SQL, text-to-Cypher, and query analysis for metadata filters are useful ways to interact with structured, graph, and vector databases respectively. NameWhen to UseDescriptionText to SQLIf users are asking questions that require information housed in a relational database, accessible via SQL.This uses an LLM to transform user input into a SQL query.Text-to-CypherIf users are asking questions that require information housed in a graph database, accessible via Cypher.This uses an LLM to transform user input into a Cypher query.Self QueryIf users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text.This uses an LLM to transform user input into two things: (1) a string to look up semantically, (2) a metadata filter to go along with it. This is useful because oftentimes questions are about the METADATA of documents (not the content itself).tipSee our blog post overview and RAG from Scratch video on query construction, the process of text-to-DSL where DSL is a domain specific language required to interact with a given database. This converts user questions into structured queries. Indexing‚ÄãFourth, consider the design of your document index. A simple and powerful idea is to decouple the documents that you index for retrieval from the documents that you pass to the LLM for generation. Indexing frequently uses embedding models with vector stores, which compress the semantic information in documents to fixed-size vectors.Many RAG approaches focus on splitting documents into chunks and retrieving some number based on similarity to an input question for the LLM. But chunk size and chunk number can be difficult to set and affect results if they do not provide full context for the LLM to answer a question. Furthermore, LLMs are increasingly capable of processing millions of tokens. Two approaches can address this tension: (1) Multi Vector retriever using an LLM to translate documents into any form (e.g., often into a summary) that is well-suited for indexing, but returns full documents to the LLM for generation. (2) ParentDocument retriever embeds document chunks, but also returns full documents. The idea is to get the best of both worlds: use concise representations (summaries or chunks) for retrieval, but use the full documents for answer generation.NameIndex TypeUses an LLMWhen to UseDescriptionVector storeVector storeNoIf you are just getting started and looking for something quick and easy.This is the simplest method and the one that is easiest to get started with. It involves creating embeddings for each piece of text.ParentDocumentVector store + Document StoreNoIf your pages have lots of smaller pieces of distinct information that are best indexed by themselves, but best retrieved all together.This involves indexing multiple chunks for each document. Then you find the chunks that are most similar in embedding space, but you retrieve the whole parent document and return that (rather than individual chunks).Multi VectorVector store + Document StoreSometimes during indexingIf you are able to extract information from documents that you think is more relevant to index than the text itself.This involves creating multiple vectors for each document. Each vector could be created in a myriad of ways - examples include summaries of the text and hypothetical questions.Time-Weighted Vector storeVector storeNoIf you have timestamps associated with your documents, and you want to retrieve the most recent onesThis fetches documents based on a combination of semantic similarity (as in normal vector retrieval) and recency (looking at timestamps of indexed documents)tipSee our RAG from Scratch video on indexing fundamentalsSee our RAG from Scratch video on multi vector retrieverFifth, consider ways to improve the quality of your similarity search itself. Embedding models compress text into fixed-length (vector) representations that capture the semantic content of the document. This compression is useful for search / retrieval, but puts a heavy burden on that single vector representation to capture the semantic nuance / detail of the document. In some cases, irrelevant or redundant content can dilute the semantic usefulness of the embedding.ColBERT is an interesting approach to address this with a higher granularity embeddings: (1) produce a contextually influenced embedding for each token in the document and query, (2) score similarity between each query token and all document tokens, (3) take the max, (4) do this for all query tokens, and (5) take the sum of the max scores (in step 3) for all query tokens to get a query-document similarity score; this token-wise scoring can yield strong results. There are some additional tricks to improve the quality of your retrieval. Embeddings excel at capturing semantic information, but may struggle with keyword-based queries. Many vector stores offer built-in hybrid-search to combine keyword and semantic similarity, which marries the benefits of both approaches. Furthermore, many vector stores have maximal marginal relevance, which attempts to diversify the results of a search to avoid returning similar and redundant documents. NameWhen to useDescriptionColBERTWhen higher granularity embeddings are needed.ColBERT uses contextually influenced embeddings for each token in the document and query to get a granular query-document similarity score.Hybrid searchWhen combining keyword-based and semantic similarity.Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches.Maximal Marginal Relevance (MMR)When needing to diversify search results.MMR attempts to diversify the results of a search to avoid returning similar and redundant documents.tipSee our RAG from Scratch video on ColBERT.Post-processing‚ÄãSixth, consider ways to filter or rank retrieved documents. This is very useful if you are combining documents returned from multiple sources, since it can can down-rank less relevant documents and / or compress similar documents. NameIndex TypeUses an LLMWhen to UseDescriptionContextual CompressionAnySometimesIf you are finding that your retrieved documents contain too much irrelevant information and are distracting the LLM.This puts a post-processing step on top of another retriever and extracts only the most relevant information from retrieved documents. This can be done with embeddings or an LLM.EnsembleAnyNoIf you have multiple retrieval methods and want to try combining them.This fetches documents from multiple retrievers and then combines them.Re-rankingAnyYesIf you want to rank retrieved documents based upon relevance, especially if you want to combine results from multiple retrieval methods .Given a query and a list of documents, Rerank indexes the documents from most to least semantically relevant to the query.tipSee our RAG from Scratch video on RAG-Fusion, on approach for post-processing across multiple queries:  Rewrite the user question from multiple perspectives, retrieve documents for each rewritten question, and combine the ranks of multiple search result lists to produce a single, unified ranking with Reciprocal Rank Fusion (RRF).Generation‚ÄãFinally, consider ways to build self-correction into your RAG system. RAG systems can suffer from low quality retrieval (e.g., if a user question is out of the domain for the index) and / or hallucinations in generation. A naive retrieve-generate pipeline has no ability to detect or self-correct from these kinds of errors. The concept of \"flow engineering\" has been introduced in the context of code generation: iteratively build an answer to a code question with unit tests to check and self-correct errors. Several works have applied this RAG, such as Self-RAG and Corrective-RAG. In both cases, checks for document relevance, hallucinations, and / or answer quality are performed in the RAG answer generation flow.We've found that graphs are a great way to reliably express logical flows and have implemented ideas from several of these papers using LangGraph, as shown in the figure below (red - routing, blue - fallback, green - self-correction):Routing: Adaptive RAG (paper). Route questions to different retrieval approaches, as discussed above Fallback: Corrective RAG (paper). Fallback to web search if docs are not relevant to querySelf-correction: Self-RAG (paper). Fix answers w/ hallucinations or don‚Äôt address questionNameWhen to useDescriptionSelf-RAGWhen needing to fix answers with hallucinations or irrelevant content.Self-RAG performs checks for document relevance, hallucinations, and answer quality during the RAG answer generation flow, iteratively building an answer and self-correcting errors.Corrective-RAGWhen needing a fallback mechanism for low relevance docs.Corrective-RAG includes a fallback (e.g., to web search) if the retrieved documents are not relevant to the query, ensuring higher quality and more relevant retrieval.tipSee several videos and cookbooks showcasing RAG with LangGraph: LangGraph Corrective RAGLangGraph combining Adaptive, Self-RAG, and Corrective RAG Cookbooks for RAG using LangGraphSee our LangGraph RAG recipes with partners:MetaMistralText splitting‚ÄãLangChain offers many different types of text splitters.\n",
            "These all live in the langchain-text-splitters package.Table columns:Name: Name of the text splitterClasses: Classes that implement this text splitterSplits On: How this text splitter splits textAdds Metadata: Whether or not this text splitter adds metadata about where each chunk came from.Description: Description of the splitter, including recommendation on when to use it.NameClassesSplits OnAdds MetadataDescriptionRecursiveRecursiveCharacterTextSplitter, RecursiveJsonSplitterA list of user defined charactersRecursively splits text. This splitting is trying to keep related pieces of text next to each other. This is the recommended way to start splitting text.HTMLHTMLHeaderTextSplitter, HTMLSectionSplitterHTML specific characters‚úÖSplits text based on HTML-specific characters. Notably, this adds in relevant information about where that chunk came from (based on the HTML)MarkdownMarkdownHeaderTextSplitter,Markdown specific characters‚úÖSplits text based on Markdown-specific characters. Notably, this adds in relevant information about where that chunk came from (based on the Markdown)Codemany languagesCode (Python, JS) specific charactersSplits text based on characters specific to coding languages. 15 different languages are available to choose from.Tokenmany classesTokensSplits text on tokens. There exist a few different ways to measure tokens.CharacterCharacterTextSplitterA user defined characterSplits text based on a user defined character. One of the simpler methods.Semantic Chunker (Experimental)SemanticChunkerSentencesFirst splits on sentences. Then combines ones next to each other if they are semantically similar enough. Taken from Greg KamradtIntegration: AI21 SemanticAI21SemanticTextSplitter‚úÖIdentifies distinct topics that form coherent pieces of text and splits along those.Evaluation‚ÄãEvaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.\n",
            "It involves testing the model's responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose.\n",
            "This process is vital for building reliable applications.LangSmith helps with this process in a few ways:It makes it easier to create and curate datasets via its tracing and annotation featuresIt provides an evaluation framework that helps you define metrics and run your app against your datasetIt allows you to track results over time and automatically run your evaluators on a schedule or as part of CI/CodeTo learn more, check out this LangSmith guide.Tracing‚ÄãA trace is essentially a series of steps that your application takes to go from input to output.\n",
            "Traces contain individual steps called runs. These can be individual calls from a model, retriever,\n",
            "tool, or sub-chains.\n",
            "Tracing gives you observability inside your chains and agents, and is vital in diagnosing issues.For a deeper dive, check out this LangSmith conceptual guide.Edit this pageWas this page helpful?You can also leave detailed feedback on GitHub.PreviousHow to create and query vector storesNextOverviewArchitecturelangchain-corePartner packageslangchainlangchain-communitylanggraphlangserveLangSmithLangChain Expression Language (LCEL)Runnable interfaceComponentsChat modelsLLMsMessagesPrompt templatesExample selectorsOutput parsersChat historyDocumentsDocument loadersText splittersEmbedding modelsVector storesRetrieversToolsToolkitsAgentsCallbacksTechniquesStreamingStructured outputRetrievalText splittingEvaluationTracingCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}